{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d08c597a9fdbf3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Knowledge Graph Embedding: DistMult embedding for Nation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9719b198c3fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from neo4j.exceptions import ClientError\n",
    "from tqdm import tqdm\n",
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d82474217c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522b3dba2a0c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_AUTH = None\n",
    "NEO4J_DB = os.environ.get(\"NEO4J_DB\", \"neo4j\")\n",
    "if os.environ.get(\"NEO4J_USER\") and os.environ.get(\"NEO4J_PASSWORD\"):\n",
    "    NEO4J_AUTH = (\n",
    "        os.environ.get(\"NEO4J_USER\"),\n",
    "        os.environ.get(\"NEO4J_PASSWORD\"),\n",
    "    )\n",
    "gds = GraphDataScience(NEO4J_URI, auth=NEO4J_AUTH, database=NEO4J_DB, arrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = gds.run_cypher(\"CREATE CONSTRAINT entity_id FOR (e:Entity) REQUIRE e.id IS UNIQUE\")\n",
    "except ClientError:\n",
    "    print(\"CONSTRAINT entity_id already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00757ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_to_id_map(data_dir, text_to_id_filename):\n",
    "    with open(data_dir + \"/\" + text_to_id_filename, \"r\") as f:\n",
    "        data = [x.split(\"\\t\") for x in f.read().split(\"\\n\")[:-1]]\n",
    "    text_to_id_map = {text: int(id) for text, id in data}\n",
    "    return text_to_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    rel_types = {\n",
    "        \"train.txt\": \"TRAIN\",\n",
    "        \"valid.txt\": \"VALID\",\n",
    "        \"test.txt\": \"TEST\",\n",
    "    }\n",
    "    url = \"https://raw.githubusercontent.com/ZhenfengLei/KGDatasets/master/Nations\"\n",
    "    data_dir = \"./Nations\"\n",
    "\n",
    "    raw_file_names = [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "    node_id_filename = \"entity2id.txt\"\n",
    "    rel_id_filename = \"relation2id.txt\"\n",
    "\n",
    "    for file in raw_file_names + [node_id_filename, rel_id_filename]:\n",
    "        if not os.path.exists(f\"{data_dir}/{file}\"):\n",
    "            os.system(f\"wget {url}/{file} -P {data_dir}\")\n",
    "\n",
    "    node_map = get_text_to_id_map(data_dir, node_id_filename)\n",
    "    rel_map = get_text_to_id_map(data_dir, rel_id_filename)\n",
    "    dataset = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    rel_split_id = {\"TRAIN\": 0, \"VALID\": 1, \"TEST\": 2}\n",
    "\n",
    "    for file_name in raw_file_names:\n",
    "        file_name_path = data_dir + \"/\" + file_name\n",
    "\n",
    "        with open(file_name_path, \"r\") as f:\n",
    "            data = [x.split(\"\\t\") for x in f.read().split(\"\\n\")[:-1]]\n",
    "\n",
    "        for i, (src_text, rel_text, dst_text) in enumerate(data):\n",
    "            source = node_map[src_text]\n",
    "            target = node_map[dst_text]\n",
    "            rel_type = \"REL_\" + rel_text.upper()\n",
    "            rel_split = rel_types[file_name]\n",
    "\n",
    "            dataset[rel_split][rel_type].append(\n",
    "                {\n",
    "                    \"source\": source,\n",
    "                    \"source_text\": src_text,\n",
    "                    \"target\": target,\n",
    "                    \"target_text\": dst_text,\n",
    "                    \"rel_type\": rel_type,\n",
    "                    \"rel_id\": rel_map[rel_text],\n",
    "                    \"rel_split\": rel_split,\n",
    "                    \"rel_split_id\": rel_split_id[rel_split],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    print(\"Number of nodes: \", len(node_map))\n",
    "    for rel_split in dataset:\n",
    "        print(\n",
    "            f\"Number of relationships of type {rel_split}: \",\n",
    "            sum([len(dataset[rel_split][rel_type]) for rel_type in dataset[rel_split]]),\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_data_in_db():\n",
    "    res = gds.run_cypher(\"MATCH (m) RETURN count(m) as num_nodes\")\n",
    "    if res[\"num_nodes\"].values[0] > 0:\n",
    "        print(\"Data already in db, number of nodes: \", res[\"num_nodes\"].values[0])\n",
    "        return\n",
    "    dataset = read_data()\n",
    "    pbar = tqdm(\n",
    "        desc=\"Putting data in db\",\n",
    "        total=sum([len(dataset[rel_split][rel_type]) for rel_split in dataset for rel_type in dataset[rel_split]]),\n",
    "    )\n",
    "\n",
    "    for rel_split in dataset:\n",
    "        for rel_type in dataset[rel_split]:\n",
    "            edges = dataset[rel_split][rel_type]\n",
    "\n",
    "            gds.run_cypher(\n",
    "                f\"\"\"\n",
    "                UNWIND $ll as l\n",
    "                MERGE (n:Entity {{id:l.source, text:l.source_text}})\n",
    "                MERGE (m:Entity {{id:l.target, text:l.target_text}})\n",
    "                MERGE (n)-[:{rel_type} {{split: l.rel_split_id, rel_id: l.rel_id}}]->(m)\n",
    "                \"\"\",\n",
    "                params={\"ll\": edges},\n",
    "            )\n",
    "            pbar.update(len(edges))\n",
    "    pbar.close()\n",
    "\n",
    "    for rel_split in dataset:\n",
    "        res = gds.run_cypher(\n",
    "            f\"\"\"\n",
    "            MATCH ()-[r:{rel_split}]->()\n",
    "            RETURN COUNT(r) AS numberOfRelationships\n",
    "            \"\"\"\n",
    "        )\n",
    "        print(f\"Number of relationships of type {rel_split} in db: \", res.numberOfRelationships)\n",
    "\n",
    "\n",
    "put_data_in_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fceb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_graphs():\n",
    "    all_rels = gds.run_cypher(\n",
    "        \"\"\"\n",
    "            CALL db.relationshipTypes() YIELD relationshipType\n",
    "        \"\"\"\n",
    "    )\n",
    "    all_rels = all_rels[\"relationshipType\"].to_list()\n",
    "    all_rels = {rel: {\"properties\": \"split\"} for rel in all_rels if rel.startswith(\"REL_\")}\n",
    "    gds.graph.drop(\"fullGraph\", failIfMissing=False)\n",
    "    gds.graph.drop(\"trainGraph\", failIfMissing=False)\n",
    "    gds.graph.drop(\"validGraph\", failIfMissing=False)\n",
    "    gds.graph.drop(\"testGraph\", failIfMissing=False)\n",
    "\n",
    "    G_full, _ = gds.graph.project(\"fullGraph\", [\"Entity\"], all_rels)\n",
    "\n",
    "    G_train, _ = gds.graph.filter(\"trainGraph\", G_full, \"*\", \"r.split = 0.0\")\n",
    "    G_valid, _ = gds.graph.filter(\"validGraph\", G_full, \"*\", \"r.split = 1.0\")\n",
    "    G_test, _ = gds.graph.filter(\"testGraph\", G_full, \"*\", \"r.split = 2.0\")\n",
    "\n",
    "    gds.graph.drop(\"fullGraph\", failIfMissing=False)\n",
    "\n",
    "    return G_train, G_valid, G_test\n",
    "\n",
    "\n",
    "G_train, G_valid, G_test = project_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.set_compute_cluster_ip(\"localhost\")\n",
    "\n",
    "model_name = \"dummyModelName_\" + str(time.time())\n",
    "\n",
    "gds.kge.model.train(\n",
    "    G_train,\n",
    "    model_name=model_name,\n",
    "    scoring_function=\"distmult\",\n",
    "    num_epochs=1,\n",
    "    embedding_dimension=10,\n",
    "    epochs_per_checkpoint=0,\n",
    "    epochs_per_val=0,\n",
    ")\n",
    "\n",
    "predict_result = gds.kge.model.predict(\n",
    "    model_name=model_name,\n",
    "    top_k=3,\n",
    "    node_ids=[\n",
    "        gds.find_node_id([\"Entity\"], {\"text\": \"brazil\"}),\n",
    "        gds.find_node_id([\"Entity\"], {\"text\": \"uk\"}),\n",
    "        gds.find_node_id([\"Entity\"], {\"text\": \"jordan\"}),\n",
    "    ],\n",
    "    rel_types=[\"REL_RELDIPLOMACY\", \"REL_RELNGO\"],\n",
    ")\n",
    "\n",
    "print(predict_result.to_string())\n",
    "#\n",
    "# gds.kge.model.predict_tail(\n",
    "#     G_train,\n",
    "#     model_name=model_name,\n",
    "#     top_k=10,\n",
    "#     node_ids=[gds.find_node_id([\"Entity\"], {\"text\": \"/m/016wzw\"}), gds.find_node_id([\"Entity\"], {\"id\": 2})],\n",
    "#     rel_types=[\"REL_1\", \"REL_2\"],\n",
    "# )\n",
    "#\n",
    "# gds.kge.model.score_triples(\n",
    "#     G_train,\n",
    "#     model_name=model_name,\n",
    "#     triples=[\n",
    "#         (gds.find_node_id([\"Entity\"], {\"text\": \"/m/016wzw\"}), \"REL_1\", gds.find_node_id([\"Entity\"], {\"id\": 2})),\n",
    "#         (gds.find_node_id([\"Entity\"], {\"id\": 0}), \"REL_123\", gds.find_node_id([\"Entity\"], {\"id\": 3})),\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786eda29280ed31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c501f8fcb411eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
